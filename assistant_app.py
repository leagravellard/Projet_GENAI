import streamlit as st
from dotenv import load_dotenv
import re
# --- Imports LangChain ---
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_community.tools import DuckDuckGoSearchRun
import wikipedia

# --- Charger les variables d'environnement ---
load_dotenv()

# --- CONFIGURATION ---
DB_PATH = "chroma_db"

# --- Initialisation du mod√®le principal ---
llm = ChatOpenAI(model_name="gpt-4o", temperature=0)

# --- Outil RAG : recherche dans la base Chroma ---
def search_documents(query: str) -> str:
    """Recherche dans les documents internes."""
    try:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
        
        # V√©rifier si la base contient des documents
        collection = db.get()
        if not collection['ids']:
            return "‚ö†Ô∏è La base de donn√©es est vide. Veuillez d'abord indexer des documents."
        
        print(f"[DEBUG RAG] Nombre de documents dans la base : {len(collection['ids'])}")
        
        # Recherche de similarit√© directe
        docs = db.similarity_search(query, k=3)
        print(f"[DEBUG RAG] Documents trouv√©s : {len(docs)}")
        
        if not docs:
            return "‚ö†Ô∏è Aucun document pertinent trouv√© dans la base."
        
        # Formater le contexte
        context = "\n\n---\n\n".join([f"Document {i+1}:\n{doc.page_content}" for i, doc in enumerate(docs)])
        print(f"[DEBUG RAG] Longueur du contexte : {len(context)} caract√®res")
        
        # Cr√©ation du prompt
        prompt = ChatPromptTemplate.from_template("""R√©ponds √† la question suivante en te basant UNIQUEMENT sur le contexte fourni.
Si le contexte ne contient pas l'information, dis-le clairement.

Contexte : {context}

Question : {question}

R√©ponse :""")
        
        # Cr√©ation de la cha√Æne simplifi√©e
        chain = prompt | llm | StrOutputParser()
        
        result = chain.invoke({"context": context, "question": query})
        print(f"[DEBUG RAG] R√©ponse g√©n√©r√©e : {result[:100]}...")
        
        return result
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"[DEBUG RAG] Erreur compl√®te :\n{error_details}")
        return f"Erreur lors de la recherche dans les documents : {str(e)}"

# --- Autres outils ---
def search_web(query: str) -> str:
    """Recherche sur Internet."""
    try:
        search = DuckDuckGoSearchRun()
        return search.run(query)
    except Exception as e:
        return f"Erreur lors de la recherche web : {str(e)}"

def search_wikipedia(query: str) -> str:
    """Recherche sur Wikipedia."""
    try:
        # Configurer la langue fran√ßaise
        wikipedia.set_lang("fr")
        
        # Rechercher la page
        try:
            # Essayer d'abord une recherche exacte
            page = wikipedia.page(query, auto_suggest=True)
        except wikipedia.exceptions.DisambiguationError as e:
            # Si plusieurs r√©sultats, prendre le premier
            page = wikipedia.page(e.options[0])
        except wikipedia.exceptions.PageError:
            # Si la page n'existe pas, chercher des suggestions
            search_results = wikipedia.search(query, results=3)
            if not search_results:
                return f"Aucun r√©sultat trouv√© sur Wikipedia pour : {query}"
            page = wikipedia.page(search_results[0])
        
        # Limiter le contenu √† 2000 caract√®res
        summary = page.summary[:2000]
        if len(page.summary) > 2000:
            summary += "..."
        
        return f"**{page.title}**\n\n{summary}\n\nüîó URL : {page.url}"
        
    except Exception as e:
        return f"Erreur lors de la recherche Wikipedia : {str(e)}"

def calculate_math(query: str) -> str:
    """Effectue des calculs math√©matiques."""
    try:
        prompt = f"R√©sous ce probl√®me math√©matique et donne uniquement le r√©sultat num√©rique : {query}"
        result = llm.invoke(prompt)
        return result.content
    except Exception as e:
        return f"Erreur de calcul : {str(e)}"

# --- Agent simplifi√© bas√© sur le LLM ---
def agent_query(user_input: str) -> str:
    """Agent qui d√©cide quel outil utiliser et r√©pond."""
    
    system_prompt = """Tu es un assistant intelligent avec acc√®s √† plusieurs outils :

1. search_documents : Pour rechercher dans les documents PDF internes
2. search_web : Pour rechercher des informations r√©centes sur Internet
3. search_wikipedia : Pour des informations encyclop√©diques
4. calculate_math : Pour effectuer des calculs math√©matiques

Pour chaque question :
- Analyse la question
- D√©cide quel outil utiliser (ou si tu peux r√©pondre directement)
- Utilise TOUJOURS search_documents en priorit√© si la question concerne des informations qui pourraient √™tre dans des documents internes

IMPORTANT : Pour utiliser un outil, tu DOIS r√©pondre EXACTEMENT dans ce format :
TOOL: nom_outil
QUERY: ta requ√™te ici

Exemples :
Question sur des documents internes ‚Üí 
TOOL: search_documents
QUERY: Quels sont les chiffres de vente ?

Question d'actualit√© ‚Üí 
TOOL: search_web
QUERY: Derni√®res actualit√©s IA

Question encyclop√©dique ‚Üí 
TOOL: search_wikipedia
QUERY: Albert Einstein

Calcul math√©matique ‚Üí 
TOOL: calculate_math
QUERY: 25 * 4 + 17

Si aucun outil n'est n√©cessaire, r√©ponds directement SANS utiliser le format TOOL/QUERY."""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_input)
    ]
    
    response = llm.invoke(messages)
    response_text = response.content
    
    # D√©tecter si l'agent veut utiliser un outil avec le nouveau format
    tool_pattern = r'TOOL:\s*(\w+)\s*\nQUERY:\s*(.+?)(?:\n|$)'
    match = re.search(tool_pattern, response_text, re.DOTALL)
    
    if match:
        tool_name = match.group(1).strip()
        tool_query = match.group(2).strip()
        
        # Ex√©cuter l'outil appropri√©
        st.info(f"üîß Utilisation de l'outil : **{tool_name}**\n\nRequ√™te : *{tool_query}*")
        
        if tool_name == "search_documents":
            tool_result = search_documents(tool_query)
        elif tool_name == "search_web":
            tool_result = search_web(tool_query)
        elif tool_name == "search_wikipedia":
            tool_result = search_wikipedia(tool_query)
        elif tool_name == "calculate_math":
            tool_result = calculate_math(tool_query)
        else:
            tool_result = f"‚ö†Ô∏è Outil '{tool_name}' non reconnu"
        
        # Demander au LLM de formuler la r√©ponse finale
        final_messages = [
            SystemMessage(content="Tu es un assistant qui formule des r√©ponses claires bas√©es sur les r√©sultats des outils. R√©ponds en fran√ßais."),
            HumanMessage(content=f"Question originale : {user_input}\n\nR√©sultat de l'outil : {tool_result}\n\nFormule une r√©ponse claire et compl√®te en fran√ßais.")
        ]
        final_response = llm.invoke(final_messages)
        return final_response.content
    
    return response_text

# --- Interface Streamlit ---
st.set_page_config(page_title="Assistant Intelligent Multi-Comp√©tences", page_icon="ü§ñ")
st.title("ü§ñ Assistant Intelligent Multi-Comp√©tences")
st.caption("Posez-moi des questions sur vos documents, le web, ou effectuez des calculs.")

# Sidebar pour les informations de debug
with st.sidebar:
    st.header("üîç Informations de Debug")
    if st.button("V√©rifier la base RAG"):
        try:
            embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
            db = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
            collection = db.get()
            st.success(f"‚úÖ Base RAG charg√©e : {len(collection['ids'])} documents")
            if collection['ids']:
                st.write("**Premiers IDs:**", collection['ids'][:5])
        except Exception as e:
            st.error(f"‚ùå Erreur : {e}")

# --- Historique de chat ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# Affichage de l'historique
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Entr√©e utilisateur
if user_query := st.chat_input("Posez votre question ici..."):
    # Ajout du message utilisateur
    st.session_state.messages.append({"role": "user", "content": user_query})
    with st.chat_message("user"):
        st.markdown(user_query)
    
    # R√©ponse de l'agent
    with st.chat_message("assistant"):
        with st.spinner("R√©flexion..."):
            try:
                answer = agent_query(user_query)
            except Exception as e:
                answer = f"‚ö†Ô∏è Une erreur est survenue : {e}"
            st.markdown(answer)
    
    # Sauvegarde de la r√©ponse
    st.session_state.messages.append({"role": "assistant", "content": answer})